----------Logistic Regression---------------

logistic regression is use to make a prediction like spam or not spam, rain or not rain

it gives a probability like 0.93 as 93% etc which can also be converted into binary 

-------Sigmoid function-----------

it is a s-shaped function

f(x) = 1 / (1+e^-x)   where e is eulers constant = 2.71828

The Curve approches at '0' starting from negative infinity and ending at positive infinity


The sigmoid functions output is between 0 and 1 it never be absolute zero or one


----------logistic regression formula---------------

z = b + w1x1 + w2x2 + ... + wnxn

z is the output called log odds and other terms same as linear regression

once done with linear regression the z value is passed into a sigmoid function to obtain probablity


y' = 1/(1+e^-z)

y' is the output of the logistic regression model
z is the linear output

----------------------

Log loss & Regularization

logistic model uses log loss as loss function instead of square loss
 
regularization is used to prevent over fitting


log loss function returns log of the magnitude of the change

----------------log loss formula----------- 

sigma(x, y) belongs D - ylog(y') - (1-y) log (1-y')


D is the dataset containing many labelled examples, which are (x, y) pairs

y is the label in a labeled example it must be 1  or 0

y' is the models prediction somewhere between 0 and 1


--------------- Regularization ------------

types of regularization

L2 regularization formula = w1^2+w2^2+....+wn^2

L2 regularization encourages weight towards 0 but never pushes it to be 0

the overall complexity will probably drop
L2 will never push weight towards 0 therefore it will not remove any features from the model


minimize(loss + complexity)

dev uses lambda to tune the overall impact of the model training by multiplying it by lambda

lambda or Regularization rate is a scalar value 

minimize (loss + regularization_rate + complexity)


A large Regularization rate will reduces the chances of overfitting

	histogram model has weights with a normal distribution and a mean weight of 0

A low Regularization rate will increase the chances of overfitting

	histogram model has weights with flat distribution

------------------

Early stopping -  simply means ending training before the model fully converges

Although early stopping usually increases training loss, it can decrease test loss.

Early stopping is a quick, but rarely optimal, form of regularization. The resulting model is very unlikely to be as good as a model trained thoroughly on the ideal regularization rate.
 	
---------------

relation of Regularization rate to learning rate

if RR > LR then : weak weights make poor predictions
if LR > RR then : the strong weight tends to produce overfit models 


 
