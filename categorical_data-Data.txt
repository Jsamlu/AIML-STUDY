------- Categorical Data -----------


Categorical data has specific set of possible values. (usually data that defines groups)

examples: 
	> different species of animals
	> weather or not an email is spam 

## numerical / numbers can also be a Categorical Data 
## True numerical data can be meaningfully multiplied  
## a good example of this is postal code


Encoding : is the process of converting the categorical data or other data into numerical vectors that model can understand.


>> Vocabulary


Dimension is synonym for number of elements in feature vector

meaning the output can be between binary data or in multiple class like data


>> if the feature of data have low number of possible categories, it is useful to encode it in to a vocabulary, the models treats each category as separate feature.

example:  colours in rainbow where each colour is given am integer value for models understanding, so that the model can understand the relations between values.

 

>> ONE-HOT ENCODING

One-hot encoding is a way of representing categorical data as a vector in which:

> one element is set to '1' and all the other element is set to '0'

it is commonly used to represent strings or identifiers that have finite set of possible values.



Country	Vector
"Denmark"	1	0	0	0	0
"Sweden"	0	1	0	0	0
"Norway"	0	0	1	0	0
"Finland"	0	0	0	1	0
"Iceland"	0	0	0	0	1


one-hot encoding is an alternative to representing data numerical data 
where using numerical data is not making sense like giving 0, 1, 2, 3 indexes to countries names

> in one-hot encoding each category will be represented as a vector of n elements
> exactly one element in one-hot vectors will have value 1.0 and the remaining will have 0.0

>>multi-one-hot vector is also in existence where element can be in multiple category do the vector will have multiples values 1.0 




>> Sparse Representation
 
a feature whose values are zero (or empty) is known as a sparse feature.

colour is a good example of sparse feature 

where rgb can have 0 ->  [1,0,0]
or in a better way for ml

-> [1,0,0,0,0,0,0,0]

blue [0,0,1,0,0,0,0,0]

since the 1 in blue is at the position 2 so the sparse representation for preceding one-hot-vector is '2'


>> Outliers in categorical data 

outliers exist in categorical data too. for example a rare colour for car such as move or avocado 

how to deal :

instead of making each outlier a different category we can make it a single "catch call" category where you can lump all the outliers call it as out-of-vocabulary.

when number of categories is high one-hot encoding is a poor choice 

Embedding modules is a better choice in such cases
	the model typically trains faster 
	the built model typically infers predications more quickly. that is ,the model has lower latency.

Hashing (hashing trick) is a less common way to reduce the number of dimensions.


